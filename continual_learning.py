# -*- coding: utf-8 -*-
"""Continual_learning (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OxEICMokY5kv9w6oPCqeoBhht4Oj3UO6
"""

import numpy as np
import pandas as pd
import os
import gc 
from tqdm import tqdm
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from sklearn.metrics import auc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

import seaborn as sns 
import matplotlib.pyplot as plt  

import torch 
import torch.nn as nn 
import torch.nn.utils.rnn as rnn_utils 
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader

MAX_SEQ = 10
epochs = 50
MAX_LR = 2e-3

dtype = {'school_id': 'int32',
         'user_id': 'int32',
         'question_id': 'int32',
         'correct': 'int16'}
#from google.colab import drive
#drive.mount('/content/gdrive')


train_df = pd.read_csv('~/test/EdNet/skill_builder_data_corrected_collapsed_org.csv', usecols=[20, 3, 5, 7], dtype=dtype)

train_df.head()

questions = train_df["problem_id"].unique()
schools = train_df["school_id"].unique()
n_questions = len(questions)
n_schools = len(schools)
print("number questions:", len(questions))
print("#of schools = ", n_schools)
print(n_questions)
type(n_questions)

group = train_df[['user_id', 'problem_id', 'correct']].groupby('user_id').apply(lambda r: (
            r['problem_id'].values,
            r['correct'].values))

#del train_df
#gc.collect()
#group.to_frame()
print("Group:",group)
var= train_df[train_df["school_id"] == 5049]
print("Var:",var)
group1 = var[['user_id', 'problem_id', 'correct']].groupby('user_id').apply(lambda r: (
            r['problem_id'].values,
            r['correct'].values))
print(len(group1))
var1= train_df[train_df["school_id"] == 1998]
group2 = var1[['user_id', 'problem_id', 'correct']].groupby('user_id').apply(lambda r: (
            r['problem_id'].values,
            r['correct'].values))
print(len(group2))

comb_df = pd.concat([var, var1])
print(comb_df)
group_comb = comb_df[['user_id', 'problem_id', 'correct']].groupby('user_id').apply(lambda r: (
            r['problem_id'].values,
            r['correct'].values))
print(len(group_comb))

school = train_df[['user_id', 'problem_id', 'school_id']].groupby('school_id').apply(lambda r: (
            r['problem_id'].values,
            r['user_id'].values))
#print(school)



import random
np.random.seed(1)
type(n_questions)

class SAKTDataset(Dataset):
    def __init__(self, group, n_questions, max_seq=MAX_SEQ):
        super(SAKTDataset, self).__init__()
        self.max_seq = max_seq
        self.n_questions = n_questions
        self.samples = group
        
        self.user_ids = [] 
        for schools in group.index:
            
            #print(group.index)
            #print(schools)
            q, qa = group[schools]
            #print("Q:", q)
            #print("QA:", qa)
            if len(q) < 2:
                continue
            
            self.user_ids.append(schools)
            

    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, index):
        
        users= train_df['user_id'].unique()
        user_id = self.user_ids[index]
        #print("Index:", index)
        #print("User ID:", user_id)
        q_, qa_ = self.samples[user_id]
        #q_, qa_ = self.samples[users]
        #print("q_, qa_", q_ , qa_)
        
        seq_len = len(q_)
        #print("seq_len", seq_len)

        q = np.zeros(self.max_seq, dtype=int)
        qa = np.zeros(self.max_seq, dtype=int)
        if seq_len >= self.max_seq:
            #if random.random()>0.1:
                #start = random.randint(0,(seq_len-self.max_seq))
                start = seq_len-self.max_seq
                end = start + self.max_seq
                #print("seq_len is greater than max_seq, start is :", start, end)
                q[:] = q_[start:end]
                qa[:] = qa_[start:end]
            #else:
                
               # q[:] = q_[-self.max_seq:]
               # qa[:] = qa_[-self.max_seq:]
            
        else:
            '''if random.random()>0.1:
                start = 0
                end = random.randint(2,seq_len)
                seq_len = end - start
                q[-seq_len:] = q_[0:seq_len]
                qa[-seq_len:] = qa_[0:seq_len]
            else:'''
                # The previous case: we store all the elements of q
            q[-seq_len:] = q_
            qa[-seq_len:] = qa_
                   
        target_id = q[1:]
        label = qa[1:]
        #print("Target:", target_id)
        #print("Label:", label)        
        
        x = np.zeros(self.max_seq-1, dtype=int)
        x = q[:-1].copy()  
        x += (qa[:-1] == 1) * self.n_questions

        #print("X:",x)
        return x, target_id, label

train1, test1 = train_test_split(group2, test_size=0.2, random_state=42)
rand, test = train_test_split(group1, test_size=0.2, random_state=42)
train , val = train_test_split(train1, test_size = 0.2, random_state=42)
#test = test1+test2
#print("A:",len(train1))
print("train:",train)
#print("test:",len(test))
#print("rand:",rand)
#print("test1:",test)
#print("val",len(val))
#print (train_df[train_df[\"school_id\"].astype(str).str.contains(5049)])
#for i in var:
#    print(\"train only that\")
    #print(i['school_id'])
train_dataset = SAKTDataset(train, n_questions)
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=8)
del train
# The same for the test dataset:
val_dataset = SAKTDataset(val, n_questions)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=8)
#del val
gc.collect()
print(len(train_dataset))
print("TD:",len(train_dataloader))
print(len(val_dataset))
print(len(test))


# The first class, FFN, defines a feed-forward network (hence, a MLP with dropout).

class FFN(nn.Module):
    def __init__(self, state_size=400):
        # This NN has only 2 layers. They are densely connected.
        # The first layer has ReLU as activation function.
        # The second layer has Dropout.
        super(FFN, self).__init__()
        self.state_size = state_size
            # state_size only refers to the number of inputs. Hence we have the same number of inputs as of hidden neurons and output neurons.
        
        self.lr1 = nn.Linear(state_size, state_size)
        self.relu = nn.ReLU()
        self.lr2 = nn.Linear(state_size, state_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)

    def forward(self, x):
        x = self.lr1(x)
        x = self.relu(x)
        x = self.lr2(x)
        x = self.relu(x)

        return self.dropout(x)


def future_mask(seq_length):
    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')
    return torch.from_numpy(future_mask)      # The we convert the matrix into a tensor (so that PyTorch can process it).

# This defines the whole SAKT model. Last layer is a FFN network.
class SAKTModel(nn.Module):
    def __init__(self, n_questions, max_seq=120, embed_dim=128):
        super(SAKTModel, self).__init__()
        self.n_questions = n_questions
        self.embed_dim = embed_dim
        
        # First argument `num_embeddings` is the size of the dictionary of embeddings.
        # The second argument `embedding_dim` is the size of each embedding vector.
        # This creates the embeddings: observe that all the embedding layers are created with the torch layer nn.Embedding.
        # from dictionary of questions to dimension d.

        max_value = train_df["problem_id"].max()
        print("MAx:", max_value)
        self.embedding = nn.Embedding(2*n_questions+max_value, embed_dim)
        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim)
        self.e_embedding = nn.Embedding(n_questions+max_value, embed_dim)
        #print(self.embedding)
        # This defines the Multihead Attention as in the paper
        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.4)
        self.dropout = nn.Dropout(0.4)
        self.layer_normal = nn.LayerNorm(embed_dim) 
        self.ffn = FFN(embed_dim)
        
        # The output is the linear of the outputs of FFN
        #self.pred = nn.Linear(embed_dim, 1)
        self.pred = nn.Linear(embed_dim, 1)
    
    def forward(self, x, question_ids):
        # x is a tensor
        # Calling x.device returns device(type='cuda', index=0)
        device = x.device
        # Transform the tensor using the embedding.
        #print("Before embedding x")
        #import pdb; pdb.set_trace()
        #print("Input to self.embedding:",x)
        x = self.embedding(x)
        #print("Embedding:", x)
        #print(x)
        # This does several things:
        # Creates a 1D tensor that goes from 0 to k, where k is the second dimension of x
        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)
        
        # This creates the embedding of the position
        pos_x = self.pos_embedding(pos_id)
        # We add the position and the input tensor x 
        x = x + pos_x
        
        # Finally, we make an embedding of the question_ids.
        e = self.e_embedding(question_ids)
        
        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]
        e = e.permute(1, 0, 2)
        
        # Creates the mask in order to avoid using the future to predict the past
        att_mask = future_mask(x.size(0)).to(device)
        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)

        att_output = self.layer_normal(att_output + e)
        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]

        x = self.ffn(att_output)
        x = self.layer_normal(x + att_output)
        #x = x.view(x.size(0), -1) 
        x = self.pred(x)
        #print("Predicted X:", x)

        return x.squeeze(-1), att_weight

# We define a function that defines each train epoch.
def train_epoch(model, dataloader, optim, scheduler, criterion, device="cpu"):
    #var= train_df[train_df["school_id"] == 5049]
    #for i in var:
    #print("train only that")
    #print(var["school_id"])
    model.train()

    train_loss = []
    num_corrects = 0
    num_total = 0
    labels = []
    outs = []
    
    tbar = tqdm(dataloader)
    #print(tbar)
    for item in dataloader:
    #for item, (input, target) in enumerate(train_iterator):
        # Gets the first item of train_iterator and sends it to the device and converts
        # it to long type.
        #print('Item:',item)
        x = item[0].to(device).long()
        #target_id = item[0][1].to(device).long()
        target_id = item[1].to(device).long()
        label = item[2].to(device).float()
        target_mask = (target_id != 0)
        #print("Target ID:", target_id)
        #print("X:", x)
          

        # set the gradients to 0.
        optim.zero_grad()
        # Get the output and weights of the model.
        #output, atten_weight = model(x, target_id)
        output, _, = model(x, target_id)
        #output = model(input_var)
        #print("output:", output)
        #print("attention_weight:", atten_weight)      
        
        loss = criterion(output, label)
        loss.backward()
        optim.step()
        scheduler.step()
        train_loss.append(loss.item())


        
        output = torch.masked_select(output, target_mask)
        label = torch.masked_select(label, target_mask)
        pred = (torch.sigmoid(output) >= 0.5).long()
        #pred1 = (torch.max(output.data, -1))
        #print(f'Pred {pred}')
        
        #print(f'Pred2 {pred2}')
        
        num_corrects += (pred == label).sum().item()
        #num_corrects += (output == label).float().sum()
        num_total += len(label)
        #print(f'Check it out {num_corrects}')

        labels.extend(label.view(-1).data.cpu().numpy())
        outs.extend(output.view(-1).data.cpu().numpy())

        #tbar.set_description('loss - {:.4f}'.format(loss))
    
    #print(f'Pred: {pred} LAbel: {label}')
    
    #print(f" VAR {num_corrects}")
    
    # Compute Accuracy
    #print("NUM_TOTAL", num_total)
    acc = num_corrects / num_total
    roc = roc_auc_score(labels, outs)#, multi_class='ovo')#,needs_proba=True)
    precision, recall, _ = precision_recall_curve(labels,outs)
    #lr_f1, lr_auc = f1_score(labels, outs), auc(prc_recall, prc_precision)
    auc_score = auc(recall, precision)
    loss = np.average(train_loss)
    
    #label = torch.masked_select(label, target_mask).cpu().numpy()
    #pred = (torch.sigmoid(output) >= 0.5).long().cpu().numpy()
    #print(metrics.confusion_matrix(label,pred))
    #print(metrics.classification_report(label,pred))

    return loss, acc, roc, auc_score

# This defines the epoch of validation.
def val_epoch(model, val_iterator, criterion, device="cpu"):
    #var= train_df[train_df["school_id"] == 5049]
    #for i in var:
    #  print("test only that")
    #model.load_state_dict(torch.load("sakt_model.pt"))
    model.eval()

    valid_loss = []
    num_corrects = 0
    num_total = 0
    labels = []
    outs = []

    tbar = tqdm(val_iterator)
    for item in tbar:
        x = item[0].to(device).long()
        target_id = item[1].to(device).long()
        label = item[2].to(device).float()
        target_mask = (target_id != 0)

        #with torch.no_grad():
            #print("Size of x:",x.size())
            #print("Size of target_id:",target_id.size())
            #output, atten_weight = model(x, target_id)
        output, _, = model(x, target_id)
        loss = criterion(output, label)
        valid_loss.append(loss.item())
        
        output = torch.masked_select(output, target_mask)
        label = torch.masked_select(label, target_mask)
        pred = (torch.sigmoid(output) >= 0.5).long()
        
        num_corrects += (pred == label).sum().item()
        num_total += len(label)

        labels.extend(label.view(-1).data.cpu().numpy())
        outs.extend(output.view(-1).data.cpu().numpy())

        tbar.set_description('loss - {:.4f}'.format(loss))

    acc = num_corrects / num_total
    roc = roc_auc_score(labels, outs)
    precision, recall, _ = precision_recall_curve(labels,outs)
    #lr_f1, lr_auc = f1_score(labels, outs), auc(prc_recall, prc_precision)
    auc_score = auc(recall, precision)
    loss = np.average(valid_loss)
    
    #print(metrics.confusion_matrix(label,pred))
    #print(metrics.classification_report(label,pred))

    return loss, acc, roc, auc_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SAKTModel(n_questions, embed_dim = 128)  #The dimension of embeddings d is 128.

optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.99, weight_decay=0.01)
#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.BCEWithLogitsLoss()
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=MAX_LR, steps_per_epoch=len(train_dataloader), epochs=epochs
)

model.to(device)
criterion.to(device)
print(model)

best_auc = 0
max_steps = 50
step = 0
for epoch in range(epochs):
  #train_loss, train_acc, auc = train_epoch(model, train_dataloader, optimizer, scheduler, criterion, device)
  #print("epoch - {}/{} train_loss - {:.3f} acc - {:.3f} auc - {:.3f} ".format(epoch+1, epochs, train_loss, train_acc, auc))
  train_loss, train_acc, roc, prc_auc = train_epoch(model, train_dataloader, optimizer, scheduler, criterion, device)
  print("epoch - {}/{} train_loss - {:.3f} acc - {:.3f} roc - {:.3f} prc_auc - {:.3f} ".format(epoch+1, epochs, train_loss, train_acc, roc, prc_auc))
  
  val_loss, avl_acc, val_roc, val_prc_auc = val_epoch(model, val_dataloader, criterion, device)
  print("epoch - {}/{} val_loss - {:.3f} acc - {:.3f} auc - {:.3f} val_prc_auc - {:.3f}".format(epoch+1, epochs, val_loss, avl_acc, val_roc, val_prc_auc))
  if roc > best_auc:
    best_auc = roc
    step = 0
    torch.save(model.state_dict(), "sakt_model.pt")
  else:
    step += 1
    if step >= max_steps:
      break

#!pip install h5py
#from keras.models import model_from_json

#torch.save(model.state_dict(), "SAKT-Capo.pt")
#output = torch.load("SAKT-Capo.pt")
#print(output)

#sakt_mod = torch.load("sakt_model.pt")
#print(sakt_mod)

#TESTING
print("Testing on 5049!!!!!!")
rand, test = train_test_split(group1, test_size=0.2, random_state=42)
#test_df = test.to_frame()
#print("Dataframe",type(test_df))
#print("Test:", test)
test_dataset = SAKTDataset(test, n_questions )
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)
#print("Length of test datset:", len(test_dataset))
#print("Dataloader:",len(test_dataloader))

import psutil
def test_epoch(model, test_iterator, criterion, device="cpu"):
  #model.load_state_dict(torch.load("sakt_model.pt"))
  model.eval()
  print("inside test_epoch !!") 
  test_loss = []
  num_corrects = 0
  num_total = 0
  labels = []
  outs = []

  prev_test_df = None
  tbar = tqdm(test_iterator)
  for item in tbar:
          x = item[0].to(device).long()
          target_id = item[1].to(device).long()
          label = item[2].to(device).float()
          target_mask = (target_id != 0)
          #print("Item", item)
          #print("Target ID:", target_id)
          #print("X:", x)

          with torch.no_grad():
            #print("Size of x:",x.size())
            #print("Size of target_id:",target_id.size())
            output, att_weight = model(x, target_id)
          
          
          output = torch.masked_select(output, target_mask)
          label = torch.masked_select(label, target_mask)

          loss = criterion(output, label)
          test_loss.append(loss.item())

          pred = (torch.sigmoid(output) >= 0.5).long()
          num_corrects += (pred == label).sum().item()
          num_total += len(label)

          labels.extend(label.squeeze(-1).data.cpu().numpy())
          outs.extend(output.view(-1).data.cpu().numpy())
          
  acc = num_corrects / num_total
  roc = roc_auc_score(labels, outs)
  precision, recall, _ = precision_recall_curve(labels,outs)
  auc_score = auc(recall, precision)
  loss = np.average(test_loss)
  
  #print(metrics.confusion_matrix(label,pred))
  #print(metrics.classification_report(label,pred))

  return loss, acc, roc, auc_score

epoch_test = 1
for epoch in range(epoch_test):
  test_loss, test_acc, test_roc, prc_auc = test_epoch(model, test_dataloader, criterion, device)
  print("epoch - {}/{} test - {:.3f} acc - {:.3f} roc - {:.3f} prc_auc - {:.3f}".format(epoch+1,epoch_test, test_loss, test_acc, test_roc, prc_auc))
  
print("Testing on 1998!!!!!!")
rand, test = train_test_split(group2, test_size=0.2, random_state=42)
#test_df = test.to_frame()
#print("Dataframe",type(test_df))
#print("Test:", test)
test_dataset = SAKTDataset(test, n_questions )
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)
#print("Length of test datset:", len(test_dataset))
#print("Dataloader:",len(test_dataloader))

for epoch in range(epoch_test):
  test_loss, test_acc, test_roc, prc_auc = test_epoch(model, test_dataloader, criterion, device)
  print("epoch - {}/{} test - {:.3f} acc - {:.3f} roc - {:.3f} prc_auc - {:.3f}".format(epoch+1,epoch_test, test_loss, test_acc, test_roc, prc_auc))
